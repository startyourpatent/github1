<!DOCTYPE html>
<!-- saved from url=(0101)https://drive.google.com/viewerng/viewer?url=https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf -->
<html lang="ko" dir="ltr" data-inboxsdk-session-id="1543550807090-0.5028720104459201"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>glorot10a.pdf</title><link rel="stylesheet" type="text/css" href="./softsignglorot10a.pdf_files/rs=AC2dHMLqvcxWo80S7Q80r76kHsKKwIsh1w"></head><body class="ndfHFb-c4YZDc-qbOKL-OEVmcd"><script src="./softsignglorot10a.pdf_files/cb=gapi(1).loaded_0" nonce="" async="" aria-hidden="true"></script><script type="text/javascript" charset="UTF-8" src="./softsignglorot10a.pdf_files/m=main" nonce="" aria-hidden="true"></script><script type="text/javascript" src="./softsignglorot10a.pdf_files/client.js.다운로드" nonce="" aria-hidden="true" gapi_processed="true"></script><script type="text/javascript" nonce="" aria-hidden="true">_init([["0",null,null,null,1,1,null,null,null,null,1,[1]
,0,null,null,"https://drive.google.com",null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,[["core-366-RC1","prod"]
,12,1,1]
,null,null,null,null,[null,"김가빈","supersynaps12@gmail.com","https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s128/photo.jpg",null,null,null,null,0]
,null,null,null,null,null,null,null,null,null,null,1,1,null,null,null,0,null,null,null,null,null,null,null,null,null,1]
,[null,"glorot10a.pdf","/viewerng/thumb?ds\u003dAON1mFyGwu5jts7OJ7DYGOAm-BQdWDJJsfu9uHBULfye3jP32SvxQxyoZrAfrdkciXFS5tLvqHLazoWQA6tWq42UJdqbRxw4N4_LhNmeKAvfeBPVLwDsewUTDVKXyHZdyEe3NLFgHhx32pfbaGJsjM91TXzp3YLPt4Kl0jPDAiorpcfkzt0M0ro7uEa0Psa2Pb8LgRXCSL6mHuA0JlT0TFWAtr4LDDMwL8gBfK5QFyjyqH10U4iY-T1_XmZZkl4mlw954DDPplje7pKowIPeWKKoKuo7fXv4NQ%3D%3D\u0026ck\u003dlantern\u0026authuser\u003d0\u0026w\u003d800\u0026webp\u003dtrue\u0026p\u003dproj",null,null,null,null,null,null,"/viewerng/upload?ds\u003dAON1mFybx8Ki_RrJckkFxTA4oY73Z8UYeFdoZ1sJCBbCczv315eM2JyxMTHmrWlMvpkn83nLpuLT7SaP3gs5JzWoNPntHsTqm3C3AO3x2ouH4llWQTfK45D0wCmAK6urGdhLXJ9jvpqbUXYdKZxh4AB2JAFi9Ni0mePvzX90xMsIcAQbnZVi0SK7CUVw_ei0653MJAjAK93ahhRaMqCdYQ9NPgwWcBKSG1yxORb73W_ZTEjw7b59bNqUlYECYfUsLVHCu2ziC07oalzAAyP3F2EkwZ5Stu-7IQ%3D%3D\u0026ck\u003dlantern\u0026authuser\u003d0\u0026p\u003dproj",null,"application/pdf",null,null,1,null,null,null,"https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf",null,1,0,null,null,null,null,null,"/viewerng/standalone/refresh?url\u003dhttps://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf",[null,null,"meta?id\u003dACFrOgC4tnnOonZLRNpzh_fUx4irn_sNJ927RCmU9tHNbbGvOZYtR_3oz5q6lbW9KbDKqBfUBAldmVcY9AiTwkbRLtcpH_hotB5D1rfqXT_D_a9n3xlQ1HzzkRo9yBfLAXJA-8I6KFa12TxkkD7x\u0026authuser\u003d0","img?id\u003dACFrOgC4tnnOonZLRNpzh_fUx4irn_sNJ927RCmU9tHNbbGvOZYtR_3oz5q6lbW9KbDKqBfUBAldmVcY9AiTwkbRLtcpH_hotB5D1rfqXT_D_a9n3xlQ1HzzkRo9yBfLAXJA-8I6KFa12TxkkD7x\u0026authuser\u003d0","press?id\u003dACFrOgC4tnnOonZLRNpzh_fUx4irn_sNJ927RCmU9tHNbbGvOZYtR_3oz5q6lbW9KbDKqBfUBAldmVcY9AiTwkbRLtcpH_hotB5D1rfqXT_D_a9n3xlQ1HzzkRo9yBfLAXJA-8I6KFa12TxkkD7x\u0026authuser\u003d0","status?id\u003dACFrOgC4tnnOonZLRNpzh_fUx4irn_sNJ927RCmU9tHNbbGvOZYtR_3oz5q6lbW9KbDKqBfUBAldmVcY9AiTwkbRLtcpH_hotB5D1rfqXT_D_a9n3xlQ1HzzkRo9yBfLAXJA-8I6KFa12TxkkD7x\u0026authuser\u003d0","https://doc-10-28-apps-viewer.googleusercontent.com/viewer/secure/pdf/3obokfm7eb6v4so4timo8djmi03j94ci/p3pvg2pb0d4jhhrhe5a4a4pg9oouu003/1543550775000/lantern/08781808740202190800/ACFrOgC4tnnOonZLRNpzh_fUx4irn_sNJ927RCmU9tHNbbGvOZYtR_3oz5q6lbW9KbDKqBfUBAldmVcY9AiTwkbRLtcpH_hotB5D1rfqXT_D_a9n3xlQ1HzzkRo9yBfLAXJA-8I6KFa12TxkkD7x",null,"presspage?id\u003dACFrOgC4tnnOonZLRNpzh_fUx4irn_sNJ927RCmU9tHNbbGvOZYtR_3oz5q6lbW9KbDKqBfUBAldmVcY9AiTwkbRLtcpH_hotB5D1rfqXT_D_a9n3xlQ1HzzkRo9yBfLAXJA-8I6KFa12TxkkD7x\u0026authuser\u003d0"]
,null,null,null,"pdf"]
,"/viewerng/standalone/du?ds\u003dAON1mFy7PoHFPUsvRus5g8sZejkOPF_1zc0r_htqpNYCByoNkR13jaaREC7-T3hJb2YwzbUPlc5KWCnL5f6pB3iXiU0tLzDUOz0aDQQMiz6XwRxM_Ibv3R3DrNWUurSJ0RaaY9NRfTZqtifrMh6PcEx7cd0F00sSfLWzbe1uN9z2Pc5mi-NaPqfXYHEZ18rpiwNdBtNLrCSje92dDFfW_tDvMupK6cMPbDB6xRLN2vo3u92NELE3HH84XvT0w3E96cnwH01OanL8W_tHVC9-gllcNt7_UzgzvA%3D%3D\u0026ck\u003dlantern\u0026t\u003dglorot10a.pdf","/viewerng/standalone/rdu?url\u003dhttps://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf",2]
);</script><div class="ndfHFb-c4YZDc ndfHFb-c4YZDc-AHmuwe-Hr88gd-OWB6Me ndfHFb-c4YZDc-vyDMJf-aZ2wEe ndfHFb-c4YZDc-i5oIFb ndfHFb-c4YZDc-TSZdd" aria-label="뷰어를 표시 중입니다." tabindex="0"><div class="ndfHFb-c4YZDc-bnBfGc ndfHFb-c4YZDc-zTETae" tabindex="0" aria-label="glorot10a.pdf 표시 중입니다."></div><div class="ndfHFb-c4YZDc-K9a4Re" style="bottom: 0px;"><div class="ndfHFb-c4YZDc-E7ORLb-LgbsSe ndfHFb-c4YZDc-LgbsSe-OWB6Me ndfHFb-c4YZDc-LgbsSe" role="button" aria-disabled="true" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="r,c" data-tooltip-offset="-6" style="user-select: none; left: 12px; display: none; opacity: 0;"><div class="ndfHFb-c4YZDc-DH6Rkf-AHe6Kc"><div class="ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-DH6Rkf-Bz112c"></div></div></div><div class="ndfHFb-c4YZDc-tJiF1e-LgbsSe ndfHFb-c4YZDc-LgbsSe-OWB6Me ndfHFb-c4YZDc-LgbsSe" role="button" aria-disabled="true" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="l,c" data-tooltip-offset="-6" style="user-select: none; right: 12px; display: none; opacity: 0;"><div class="ndfHFb-c4YZDc-DH6Rkf-AHe6Kc"><div class="ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-DH6Rkf-Bz112c"></div></div></div><div class="ndfHFb-c4YZDc-q77wGc" style="opacity: 0;"><div class="ndfHFb-c4YZDc-DARUcf-NnAfwf-i5oIFb" style="" aria-label=" 1/8페이지"><div class="ndfHFb-c4YZDc-DARUcf-NnAfwf-tJHJj"></div><div class="ndfHFb-c4YZDc-DARUcf-NnAfwf-cQYSPc">1</div><span class="ndfHFb-c4YZDc-DARUcf-NnAfwf-hgDUwe">/</span><div class="ndfHFb-c4YZDc-DARUcf-NnAfwf-j4LONd">8페이지</div></div><div class="ndfHFb-c4YZDc-nJjxad-nK2kYb-i5oIFb" style=""><div class="ndfHFb-c4YZDc-to915-LgbsSe ndfHFb-c4YZDc-nJjxad-m9bMae-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-LgbsSe-OWB6Me" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="축소" data-tooltip="축소" style="user-select: none;" aria-disabled="true"><div class="ndfHFb-c4YZDc-Bz112c"></div></div><div class="ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-to915-LgbsSe ndfHFb-c4YZDc-nJjxad-hj4D6d-LgbsSe VIpgJd-TzA9Ye-eEGnhe" role="button" tabindex="0" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" style="user-select: none;" aria-label="폭에 맞춤" data-tooltip="폭에 맞춤"><div class="ndfHFb-c4YZDc-Bz112c"></div></div><div class="ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-to915-LgbsSe ndfHFb-c4YZDc-nJjxad-bEDTcc-LgbsSe VIpgJd-TzA9Ye-eEGnhe" role="button" tabindex="0" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="확대" data-tooltip="확대" style="user-select: none;"><div class="ndfHFb-c4YZDc-Bz112c"></div></div></div><div class="ndfHFb-c4YZDc-LzGo7c" style="display: none;"></div></div><div class="ndfHFb-c4YZDc-K9a4Re-nKQ6qf ndfHFb-c4YZDc-TvD9Pc-qnnXGd" role="main" style=""><div class="ndfHFb-c4YZDc-EglORb-ge6pde ndfHFb-c4YZDc-K9a4Re-ge6pde-Ne3sFf" role="status" tabindex="-1" aria-label="로드 중" style="display: none;"><div class="ndfHFb-c4YZDc-EglORb-ge6pde-RJLb9c ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"><div class="ndfHFb-aZ2wEe"><div class="ndfHFb-vyDMJf-aZ2wEe auswjd"><div class="aZ2wEe-pbTTYe aZ2wEe-v3pZbf"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-oq6NAc"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-gS7Ybc"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-nllRtd"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div></div></div></div><span class="ndfHFb-c4YZDc-EglORb-ge6pde-fmcmS ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae" aria-hidden="true">로드 중…</span></div><div class="ndfHFb-c4YZDc-cYSp0e ndfHFb-c4YZDc-oKVyEf" style=""><textarea class="ndfHFb-c4YZDc-cYSp0e-B7I4Od" aria-hidden="true" tabindex="-1"></textarea><div class="ndfHFb-c4YZDc-cYSp0e-s2gQvd ndfHFb-c4YZDc-s2gQvd" tabindex="-1" style="margin-left: 12px;"><div class="ndfHFb-c4YZDc-cYSp0e-Oz6c3e ndfHFb-c4YZDc-cYSp0e-DARUcf-gSKZZ ndfHFb-c4YZDc-neVct-RCfa3e" role="document" tabindex="0" style="margin-top: 56px; margin-bottom: 56px; width: 680px; left: 0px;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"><h2 class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-tJHJj" tabindex="0">&nbsp;1/8페이지</h2><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 47.0588%; top: 95.7071%; width: 7.02614%; height: 1.76768%;">249
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 14.5425%; top: 11.6162%; width: 75%; height: 2.0202%;">Understanding the difficulty of training deep feedforward neural networks
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 30.0654%; top: 19.9495%; width: 44.281%; height: 1.51515%;">Xavier Glorot Yoshua Bengio
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 32.3529%; top: 21.4646%; width: 38.5621%; height: 1.51515%;">DIRO, Universite de Montr  ́ eal, Montr  ́ eal, Qu  ́ ebec, Canada  ́
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 27.6144%; top: 25.7576%; width: 7.67974%; height: 1.38889%;">Abstract
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 29.4192%; width: 32.1895%; height: 3.0303%;">Whereas before 2006 it appears that deep multi-
layer neural networks were not successfully
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 32.4495%; width: 32.1895%; height: 1.51515%;">trained, since then several algorithms have been
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 33.9646%; width: 32.1895%; height: 3.0303%;">shown to successfully train them, with experi-
mental results showing the superiority of deeper
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 36.9949%; width: 32.1895%; height: 2.77778%;">vs less deep architectures. All these experimen-
tal results were obtained with new initialization
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 40.0253%; width: 32.1895%; height: 1.51515%;">or training mechanisms. Our objective here is to
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 41.5404%; width: 32.1895%; height: 1.51515%;">understand better why standard gradient descent
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 43.0556%; width: 32.1895%; height: 1.51515%;">from random initialization is doing so poorly
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 44.5707%; width: 32.1895%; height: 1.51515%;">with deep neural networks, to better understand
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 46.0859%; width: 32.1895%; height: 1.51515%;">these recent relative successes and help design
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 47.601%; width: 32.1895%; height: 1.51515%;">better algorithms in the future. We first observe
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 49.1162%; width: 32.1895%; height: 3.0303%;">the influence of the non-linear activations func-
tions. We find that the logistic sigmoid activation
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 52.1465%; width: 32.1895%; height: 2.90404%;">is unsuited for deep networks with random ini-
tialization because of its mean value, which can
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 55.1768%; width: 32.1895%; height: 2.90404%;">drive especially the top hidden layer into satu-
ration. Surprisingly, we find that saturated units
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 58.0808%; width: 32.1895%; height: 1.51515%;">can move out of saturation by themselves, albeit
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.5229%; top: 59.596%; width: 32.0261%; height: 1.51515%;">slowly, and explaining the plateaus sometimes
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.5229%; top: 61.1111%; width: 32.0261%; height: 1.51515%;">seen when training neural networks. We find that
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 62.6263%; width: 32.1895%; height: 1.51515%;">a new non-linearity that saturates less can often
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 64.1414%; width: 32.1895%; height: 1.51515%;">be beneficial. Finally, we study how activations
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 65.6566%; width: 32.1895%; height: 4.54545%;">and gradients vary across layers and during train-
ing, with the idea that training may be more dif-
ficult when the singular values of the Jacobian
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 70.202%; width: 32.1895%; height: 1.51515%;">associated with each layer are far from 1. Based
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 71.7172%; width: 32.1895%; height: 3.0303%;">on these considerations, we propose a new ini-
tialization scheme that brings substantially faster
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.3595%; top: 75%; width: 8.82353%; height: 1.26263%;">convergence.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 79.0404%; width: 22.2222%; height: 1.76768%;">1 Deep Neural Networks
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 81.9444%; width: 38.7255%; height: 1.51515%;">Deep learning methods aim at learning feature hierarchies
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 83.4596%; width: 38.7255%; height: 1.51515%;">with features from higher levels of the hierarchy formed
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 84.9747%; width: 38.7255%; height: 1.51515%;">by the composition of lower level features. They include
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 87.6263%; width: 39.0523%; height: 1.64141%;">Appearing in Proceedings of the 13th International Conference
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 89.0152%; width: 38.5621%; height: 3.91414%;">on Artificial Intelligence and Statistics (AISTATS) 2010, Chia La-
guna Resort, Sardinia, Italy. Volume 9 of JMLR: W&amp;CP 9. Copy-
right 2010 by the authors.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 25.8838%; width: 38.7255%; height: 1.51515%;">learning methods for a wide array of deep architectures,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 27.399%; width: 38.8889%; height: 3.0303%;">including neural networks with many hidden layers (Vin-
cent et al., 2008) and graphical models with many levels of
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 30.4293%; width: 38.7255%; height: 1.51515%;">hidden variables (Hinton et al., 2006), among others (Zhu
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 31.9444%; width: 38.7255%; height: 4.54545%;">et al., 2009; Weston et al., 2008). Much attention has re-
cently been devoted to them (see (Bengio, 2009) for a re-
view), because of their theoretical appeal, inspiration from
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 36.4899%; width: 38.7255%; height: 1.51515%;">biology and human cognition, and because of empirical
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 38.0051%; width: 38.5621%; height: 1.51515%;">success in vision (Ranzato et al., 2007; Larochelle et al.,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 39.5202%; width: 38.7255%; height: 3.0303%;">2007; Vincent et al., 2008) and natural language process-
ing (NLP) (Collobert &amp; Weston, 2008; Mnih &amp; Hinton,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 42.5505%; width: 38.7255%; height: 4.54545%;">2009). Theoretical results reviewed and discussed by Ben-
gio (2009), suggest that in order to learn the kind of com-
plicated functions that can represent high-level abstractions
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 47.096%; width: 38.5621%; height: 1.51515%;">(e.g. in vision, language, and other AI-level tasks), one
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 48.6111%; width: 19.4444%; height: 1.51515%;">may need deep architectures.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 50.8838%; width: 38.7255%; height: 2.77778%;">Most of the recent experimental results with deep archi-
tecture are obtained with models that can be turned into
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 53.9141%; width: 38.7255%; height: 1.38889%;">deep supervised neural networks, but with initialization or
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 55.303%; width: 38.7255%; height: 1.51515%;">training schemes different from the classical feedforward
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 56.8182%; width: 38.7255%; height: 1.51515%;">neural networks (Rumelhart et al., 1986). Why are these
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 58.3333%; width: 38.7255%; height: 1.51515%;">new algorithms working so much better than the standard
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 59.8485%; width: 38.7255%; height: 1.51515%;">random initialization and gradient-based optimization of a
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 61.3636%; width: 38.5621%; height: 1.51515%;">supervised training criterion? Part of the answer may be
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 62.8788%; width: 38.8889%; height: 4.54545%;">found in recent analyses of the effect of unsupervised pre-
training (Erhan et al., 2009), showing that it acts as a regu-
larizer that initializes the parameters in a “better” basin of
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 67.4242%; width: 38.7255%; height: 1.51515%;">attraction of the optimization procedure, corresponding to
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 68.9394%; width: 38.7255%; height: 3.0303%;">an apparent local minimum associated with better general-
ization. But earlier work (Bengio et al., 2007) had shown
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 71.9697%; width: 38.7255%; height: 4.54545%;">that even a purely supervised but greedy layer-wise proce-
dure would give better results. So here instead of focus-
ing on what unsupervised pre-training or semi-supervised
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 76.5152%; width: 38.7255%; height: 1.51515%;">criteria bring to deep architectures, we focus on analyzing
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 78.0303%; width: 38.2353%; height: 3.0303%;">what may be going wrong with good old (but deep) multi-
layer neural networks.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 81.8182%; width: 38.7255%; height: 3.0303%;">Our analysis is driven by investigative experiments to mon-
itor activations (watching for saturation of hidden units)
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 84.8485%; width: 38.5621%; height: 1.51515%;">and gradients, across layers and across training iterations.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 86.3636%; width: 38.7255%; height: 5.93434%;">We also evaluate the effects on these of choices of acti-
vation function (with the idea that it might affect satura-
tion) and initialization procedure (since unsupervised pre-
training is a particular form of initialization and it has a
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 92.298%; width: 10.4575%; height: 1.51515%;">drastic impact).
</p></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div><img src="blob:https://drive.google.com/cc6163ae-60a9-4475-9ac7-3754030a9239" class="ndfHFb-c4YZDc-cYSp0e-DARUcf-RJLb9c" alt=" 1/8페이지" aria-hidden="true"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"><a href="http://www.iro.umontreal/" target="_blank" class="ndfHFb-c4YZDc-cYSp0e-DARUcf-hSRGPd" tabindex="0" role="link" aria-label="http://www.iro.umontreal" data-saferedirecturl="https://www.google.com/url?q=http://www.iro.umontreal&amp;ust=1543635960000000&amp;usg=AFQjCNFYM-2fOnXJe0WkrUVT-rvfHojTfg&amp;hl=ko" rel="noreferrer" style="left: 25.9804%; top: 13.7626%; width: 23.6928%; height: 1.38889%;"></a><a href="http://www.image-net.org/" target="_blank" class="ndfHFb-c4YZDc-cYSp0e-DARUcf-hSRGPd" tabindex="0" role="link" aria-label="http://www.image-net.org" data-saferedirecturl="https://www.google.com/url?q=http://www.image-net.org&amp;ust=1543635960000000&amp;usg=AFQjCNHOqHH8O3SeC2KAnhpsvzLIlSnjCQ&amp;hl=ko" rel="noreferrer" style="left: 61.7647%; top: 66.7929%; width: 24.0196%; height: 1.38889%;"></a></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"><h2 class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-tJHJj" tabindex="0">&nbsp;2/8페이지</h2><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 47.0588%; top: 95.7071%; width: 7.02614%; height: 1.76768%;">250
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 28.5948%; top: 5.80808%; width: 46.8954%; height: 1.38889%;">Understanding the difficulty of training deep feedforward neural networks
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 9.09091%; width: 32.3529%; height: 1.76768%;">2 Experimental Setting and Datasets
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 12.2475%; width: 38.7255%; height: 1.51515%;">Code to produce the new datasets introduced in this section
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 13.7626%; width: 38.3987%; height: 1.38889%;">is available from: http://www.iro.umontreal.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 15.2778%; width: 34.3137%; height: 1.38889%;">ca/ ̃lisa/twiki/bin/view.cgi/Public/
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 16.7929%; width: 24.0196%; height: 1.38889%;">DeepGradientsAISTATS2010.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 19.9495%; width: 31.5359%; height: 1.38889%;">2.1 Online Learning on an Infinite Dataset:
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.8497%; top: 21.3384%; width: 10.6209%; height: 1.51515%;">Shapeset-3 × 2
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 23.8636%; width: 38.7255%; height: 3.0303%;">Recent work with deep architectures (see Figure 7 in Ben-
gio (2009)) shows that even with very large training sets
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 26.8939%; width: 38.7255%; height: 3.0303%;">or online learning, initialization from unsupervised pre-
training yields substantial improvement, which does not
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 29.9242%; width: 38.7255%; height: 1.51515%;">vanish as the number of training examples increases. The
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 31.4394%; width: 38.7255%; height: 1.51515%;">online setting is also interesting because it focuses on the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 32.9545%; width: 38.7255%; height: 4.54545%;">optimization issues rather than on the small-sample regu-
larization effects, so we decided to include in our experi-
ments a synthetic images dataset inspired from Larochelle
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 37.5%; width: 38.7255%; height: 1.38889%;">et al. (2007) and Larochelle et al. (2009), from which as
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 39.0152%; width: 38.7255%; height: 1.51515%;">many examples as needed could be sampled, for testing the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 40.5303%; width: 16.5033%; height: 1.38889%;">online learning scenario.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 42.6768%; width: 38.7255%; height: 4.54545%;">We call this dataset the Shapeset-3 × 2 dataset, with ex-
ample images in Figure 1 (top). Shapeset-3 × 2 con-
tains images of 1 or 2 two-dimensional objects, each taken
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 47.2222%; width: 38.7255%; height: 1.51515%;">from 3 shape categories (triangle, parallelogram, ellipse),
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 48.7374%; width: 38.7255%; height: 1.51515%;">and placed with random shape parameters (relative lengths
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 50.2525%; width: 38.5621%; height: 1.51515%;">and/or angles), scaling, rotation, translation and grey-scale.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 52.5253%; width: 38.7255%; height: 1.51515%;">We noticed that for only one shape present in the image the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 54.0404%; width: 38.7255%; height: 1.51515%;">task of recognizing it was too easy. We therefore decided
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 55.5556%; width: 38.7255%; height: 1.51515%;">to sample also images with two objects, with the constraint
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 57.0707%; width: 38.7255%; height: 1.51515%;">that the second object does not overlap with the first by
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 58.5859%; width: 38.7255%; height: 4.54545%;">more than fifty percent of its area, to avoid hiding it en-
tirely. The task is to predict the objects present (e.g. trian-
gle + ellipse, parallelogram + parallelogram, triangle alone,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 63.1313%; width: 38.7255%; height: 1.51515%;">etc.) without having to distinguish between the foreground
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 64.6465%; width: 38.5621%; height: 1.51515%;">shape and the background shape when they overlap. This
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 66.1616%; width: 28.9216%; height: 1.51515%;">therefore defines nine configuration classes.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 68.4343%; width: 38.7255%; height: 3.0303%;">The task is fairly difficult because we need to discover in-
variances over rotation, translation, scaling, object color,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 71.4646%; width: 38.7255%; height: 1.51515%;">occlusion and relative position of the shapes. In parallel we
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 72.9798%; width: 38.7255%; height: 1.51515%;">need to extract the factors of variability that predict which
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 74.4949%; width: 16.9935%; height: 1.38889%;">object shapes are present.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 76.6414%; width: 38.7255%; height: 1.51515%;">The size of the images are arbitrary but we fixed it to 32×32
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 78.1566%; width: 35.4575%; height: 1.51515%;">in order to work with deep dense networks efficiently.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 81.3131%; width: 14.5425%; height: 1.26263%;">2.2 Finite Datasets
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 83.8384%; width: 38.7255%; height: 1.51515%;">The MNIST digits (LeCun et al., 1998a), dataset has
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 85.3535%; width: 38.7255%; height: 1.51515%;">50,000 training images, 10,000 validation images (for
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 86.8687%; width: 38.7255%; height: 1.51515%;">hyper-parameter selection), and 10,000 test images, each
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 88.3838%; width: 38.5621%; height: 1.51515%;">showing a 28×28 grey-scale pixel image of one of the 10
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 89.899%; width: 4.41176%; height: 1.51515%;">digits.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 40.5303%; width: 79.902%; height: 53.1566%;">CIFAR-10 (Krizhevsky &amp; Hinton, 2009) is a labelled sub-
Figure 1: Top: Shapeset-3×2 images at 64×64 resolution.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 42.0455%; width: 38.7255%; height: 1.51515%;">The examples we used are at 32×32 resolution. The learner
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 43.5606%; width: 38.8889%; height: 4.54545%;">tries to predict which objects (parallelogram, triangle, or el-
lipse) are present, and 1 or 2 objects can be present, yield-
ing 9 possible classifications. Bottom: Small-ImageNet
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 47.9798%; width: 16.6667%; height: 1.51515%;">images at full resolution.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 50.8838%; width: 38.5621%; height: 1.51515%;">set of the tiny-images dataset that contains 50,000 training
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 52.399%; width: 38.7255%; height: 1.51515%;">examples (from which we extracted 10,000 as validation
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 53.9141%; width: 38.5621%; height: 4.54545%;">data) and 10,000 test examples. There are 10 classes cor-
responding to the main object in each image: airplane, au-
tomobile, bird, cat, deer, dog, frog, horse, ship, or truck.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 58.4596%; width: 38.7255%; height: 1.51515%;">The classes are balanced. Each image is in color, but is
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.1046%; top: 59.9747%; width: 39.0523%; height: 1.51515%;">just 32 × 32 pixels in size, so the input is a vector of
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 61.4899%; width: 21.4052%; height: 1.26263%;">32 × 32 × 3 = 3072 real values.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 63.7626%; width: 38.5621%; height: 1.51515%;">Small-ImageNet which is a set of tiny 37×37 gray level
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 65.1515%; width: 38.7255%; height: 1.51515%;">images dataset computed from the higher-resolution and
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 66.6667%; width: 38.7255%; height: 3.0303%;">larger set at http://www.image-net.org, with la-
bels from the WordNet noun hierarchy. We have used
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 69.697%; width: 38.7255%; height: 1.51515%;">90,000 examples for training, 10,000 for the validation set,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 71.2121%; width: 38.7255%; height: 2.90404%;">and 10,000 for testing. There are 10 balanced classes: rep-
tiles, vehicles, birds, mammals, fish, furniture, instruments,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 74.2424%; width: 38.7255%; height: 1.51515%;">tools, flowers and fruits Figure 1 (bottom) shows randomly
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 75.7576%; width: 11.9281%; height: 1.51515%;">chosen examples.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 78.9141%; width: 18.9542%; height: 1.51515%;">2.3 Experimental Setting
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 81.4394%; width: 38.7255%; height: 1.51515%;">We optimized feedforward neural networks with one to
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 82.9545%; width: 38.7255%; height: 1.51515%;">five hidden layers, with one thousand hidden units per
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 84.4697%; width: 38.7255%; height: 3.0303%;">layer, and with a softmax logistic regression for the out-
put layer. The cost function is the negative log-likelihood
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 87.3737%; width: 38.5621%; height: 1.64141%;">− log P(y|x), where (x, y) is the (input image, target class)
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 89.0152%; width: 38.7255%; height: 1.38889%;">pair. The neural networks were optimized with stochastic
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 90.404%; width: 38.2353%; height: 3.28283%;">back-propagation on mini-batches of size ten, i.e., the av-
erage g of ∂−log P (y|x)
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 63.7255%; top: 92.1717%; width: 28.268%; height: 1.64141%;">∂θ was computed over 10 consecutive
</p></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div><img src="blob:https://drive.google.com/ec5251c5-9948-4741-a9ea-70ed1a48a953" class="ndfHFb-c4YZDc-cYSp0e-DARUcf-RJLb9c" alt=" 2/8페이지" aria-hidden="true"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"><h2 class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-tJHJj" tabindex="0">&nbsp;3/8페이지</h2><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 47.0588%; top: 95.7071%; width: 6.86275%; height: 1.76768%;">251
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 42.3203%; top: 5.80808%; width: 19.4444%; height: 1.38889%;">Xavier Glorot, Yoshua Bengio
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 9.21717%; width: 38.7255%; height: 1.64141%;">training pairs (x, y) and used to update parameters θ in that
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 10.7323%; width: 38.7255%; height: 3.15657%;">direction, with θ ← θ − g. The learning rate  is a hyper-
parameter that is optimized based on validation set error
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 13.7626%; width: 28.4314%; height: 1.51515%;">after a large number of updates (5 million).
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 16.0354%; width: 38.7255%; height: 1.51515%;">We varied the type of non-linear activation function in the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 17.5505%; width: 25.4902%; height: 1.64141%;">hidden layers: the sigmoid 1/(1 + e
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 37.2549%; top: 17.6768%; width: 2.12418%; height: 0.757576%;">−x
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 39.0523%; top: 17.5505%; width: 11.7647%; height: 1.64141%;">), the hyperbolic
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 19.0657%; width: 38.5621%; height: 3.15657%;">tangent tanh(x), and a newly proposed activation func-
tion (Bergstra et al., 2009) called the softsign, x/(1 + |x|).
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 22.096%; width: 38.7255%; height: 1.51515%;">The softsign is similar to the hyperbolic tangent (its range
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 23.6111%; width: 38.7255%; height: 1.51515%;">is -1 to 1) but its tails are quadratic polynomials rather
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 25.1263%; width: 38.7255%; height: 1.51515%;">than exponentials, i.e., it approaches its asymptotes much
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 26.6414%; width: 4.90196%; height: 1.26263%;">slower.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 28.9141%; width: 38.7255%; height: 3.0303%;">In the comparisons, we search for the best hyper-
parameters (learning rate and depth) separately for each
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 31.9444%; width: 38.7255%; height: 1.51515%;">model. Note that the best depth was always five for
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 33.4596%; width: 38.5621%; height: 1.51515%;">Shapeset-3 × 2, except for the sigmoid, for which it was
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 34.9747%; width: 3.43137%; height: 1.26263%;">four.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 37.2475%; width: 38.7255%; height: 1.64141%;">We initialized the biases to be 0 and the weights Wij at
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 38.7626%; width: 36.6013%; height: 1.51515%;">each layer with the following commonly used heuristic:
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 23.2026%; top: 41.1616%; width: 6.53595%; height: 1.64141%;">Wij ∼ U
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 29.5752%; top: 40.5303%; width: 0.816993%; height: 2.65152%;">h
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 30.5556%; top: 41.6667%; width: 1.47059%; height: 0.378788%;">−
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 33.0065%; top: 40.2778%; width: 1.14379%; height: 1.26263%;">1
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 32.3529%; top: 41.9192%; width: 1.63399%; height: 1.64141%;">√
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 33.6601%; top: 42.298%; width: 1.30719%; height: 1.0101%;">n
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 34.8039%; top: 41.9192%; width: 0.653595%; height: 0.757576%;">,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 36.6013%; top: 40.2778%; width: 0.980392%; height: 1.26263%;">1
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 35.7843%; top: 41.9192%; width: 1.79739%; height: 1.64141%;">√
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 37.0915%; top: 42.298%; width: 1.30719%; height: 1.0101%;">n
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 38.2353%; top: 40.5303%; width: 0.816993%; height: 2.65152%;">i
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 39.0523%; top: 41.1616%; width: 11.7647%; height: 1.51515%;">, (1)
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 44.1919%; width: 38.7255%; height: 1.51515%;">where U[−a, a] is the uniform distribution in the interval
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 45.5808%; width: 38.5621%; height: 1.64141%;">(−a, a) and n is the size of the previous layer (the number
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 47.2222%; width: 12.5817%; height: 1.51515%;">of columns of W).
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 49.8737%; width: 32.0261%; height: 1.51515%;">3 Effect of Activation Functions and
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 15.0327%; top: 51.6414%; width: 23.5294%; height: 1.76768%;">Saturation During Training
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 54.5455%; width: 38.7255%; height: 1.51515%;">Two things we want to avoid and that can be revealed from
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 56.0606%; width: 38.7255%; height: 4.41919%;">the evolution of activations is excessive saturation of acti-
vation functions on one hand (then gradients will not prop-
agate well), and overly linear units (they will not compute
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 60.4798%; width: 15.1961%; height: 1.51515%;">something interesting).
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 63.3838%; width: 25.1634%; height: 1.51515%;">3.1 Experiments with the Sigmoid
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 65.6566%; width: 38.7255%; height: 1.51515%;">The sigmoid non-linearity has been already shown to slow
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 67.1717%; width: 38.7255%; height: 1.51515%;">down learning because of its none-zero mean that induces
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 68.6869%; width: 38.7255%; height: 1.51515%;">important singular values in the Hessian (LeCun et al.,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 70.202%; width: 38.5621%; height: 1.51515%;">1998b). In this section we will see another symptomatic
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 71.7172%; width: 38.7255%; height: 1.51515%;">behavior due to this activation function in deep feedforward
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 73.2323%; width: 6.69935%; height: 1.26263%;">networks.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 75.5051%; width: 38.7255%; height: 3.0303%;">We want to study possible saturation, by looking at the evo-
lution of activations during training, and the figures in this
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 78.5354%; width: 38.7255%; height: 3.0303%;">section show results on the Shapeset-3 × 2 data, but sim-
ilar behavior is observed with the other datasets. Figure 2
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 81.5657%; width: 38.7255%; height: 4.54545%;">shows the evolution of the activation values (after the non-
linearity) at each hidden layer during training of a deep ar-
chitecture with sigmoid activation functions. Layer 1 refers
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 86.1111%; width: 38.7255%; height: 1.51515%;">to the output of first hidden layer, and there are four hidden
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 87.6263%; width: 38.7255%; height: 1.51515%;">layers. The graph shows the means and standard deviations
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 89.1414%; width: 38.7255%; height: 1.51515%;">of these activations. These statistics along with histograms
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 90.6566%; width: 38.7255%; height: 1.51515%;">are computed at different times during learning, by looking
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 92.1717%; width: 36.2745%; height: 1.51515%;">at activation values for a fixed set of 300 test examples.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 19.8232%; width: 38.7255%; height: 1.51515%;">Figure 2: Mean and standard deviation (vertical bars) of the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 21.3384%; width: 38.5621%; height: 1.38889%;">activation values (output of the sigmoid) during supervised
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 22.7273%; width: 38.7255%; height: 4.54545%;">learning, for the different hidden layers of a deep archi-
tecture. The top hidden layer quickly saturates at 0 (slow-
ing down all learning), but then slowly desaturates around
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 27.2727%; width: 7.51634%; height: 1.51515%;">epoch 100.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 31.0606%; width: 38.7255%; height: 1.38889%;">We see that very quickly at the beginning, all the sigmoid
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 32.4495%; width: 38.7255%; height: 1.51515%;">activation values of the last hidden layer are pushed to their
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 33.9646%; width: 38.7255%; height: 1.51515%;">lower saturation value of 0. Inversely, the others layers
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 35.4798%; width: 38.7255%; height: 3.0303%;">have a mean activation value that is above 0.5, and decreas-
ing as we go from the output layer to the input layer. We
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 38.5101%; width: 38.7255%; height: 1.51515%;">have found that this kind of saturation can last very long in
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 40.0253%; width: 38.7255%; height: 3.0303%;">deeper networks with sigmoid activations, e.g., the depth-
five model never escaped this regime during training. The
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 43.0556%; width: 38.7255%; height: 3.0303%;">big surprise is that for intermediate number of hidden lay-
ers (here four), the saturation regime may be escaped. At
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 46.0859%; width: 38.7255%; height: 3.0303%;">the same time that the top hidden layer moves out of satura-
tion, the first hidden layer begins to saturate and therefore
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 49.1162%; width: 7.84314%; height: 1.26263%;">to stabilize.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 51.3889%; width: 38.7255%; height: 2.77778%;">We hypothesize that this behavior is due to the combina-
tion of random initialization and the fact that an hidden unit
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 54.4192%; width: 38.7255%; height: 1.51515%;">output of 0 corresponds to a saturated sigmoid. Note that
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 55.9343%; width: 38.7255%; height: 3.0303%;">deep networks with sigmoids but initialized from unsuper-
vised pre-training (e.g. from RBMs) do not suffer from
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 58.9646%; width: 38.7255%; height: 1.51515%;">this saturation behavior. Our proposed explanation rests on
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 60.4798%; width: 38.7255%; height: 1.51515%;">the hypothesis that the transformation that the lower layers
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 61.9949%; width: 38.7255%; height: 1.51515%;">of the randomly initialized network computes initially is
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 63.5101%; width: 38.7255%; height: 2.90404%;">not useful to the classification task, unlike the transforma-
tion obtained from unsupervised pre-training. The logistic
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 66.4141%; width: 38.7255%; height: 1.64141%;">layer output softmax(b+W h) might initially rely more on
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 67.9293%; width: 38.7255%; height: 1.51515%;">its biases b (which are learned very quickly) than on the top
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 69.4444%; width: 38.7255%; height: 1.51515%;">hidden activations h derived from the input image (because
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 70.9596%; width: 38.5621%; height: 1.51515%;">h would vary in ways that are not predictive of y, maybe
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 72.4747%; width: 38.7255%; height: 1.51515%;">correlated mostly with other and possibly more dominant
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 73.9899%; width: 38.7255%; height: 1.51515%;">variations of x). Thus the error gradient would tend to
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 75.5051%; width: 38.7255%; height: 1.51515%;">push W h towards 0, which can be achieved by pushing
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 77.0202%; width: 38.5621%; height: 1.51515%;">h towards 0. In the case of symmetric activation functions
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 78.5354%; width: 38.7255%; height: 1.51515%;">like the hyperbolic tangent and the softsign, sitting around
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 80.0505%; width: 38.5621%; height: 1.51515%;">0 is good because it allows gradients to flow backwards.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 81.5657%; width: 38.7255%; height: 1.51515%;">However, pushing the sigmoid outputs to 0 would bring
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 83.0808%; width: 38.7255%; height: 3.0303%;">them into a saturation regime which would prevent gradi-
ents to flow backward and prevent the lower layers from
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 86.1111%; width: 38.7255%; height: 1.51515%;">learning useful features. Eventually but slowly, the lower
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 87.6263%; width: 38.7255%; height: 1.51515%;">layers move toward more useful features and the top hidden
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 89.1414%; width: 38.7255%; height: 2.90404%;">layer then moves out of the saturation regime. Note how-
ever that, even after this, the network moves into a solution
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 92.1717%; width: 38.7255%; height: 1.51515%;">that is of poorer quality (also in terms of generalization)
</p></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div><img src="blob:https://drive.google.com/c677ce4c-9999-4414-a433-72c4bd40ee61" class="ndfHFb-c4YZDc-cYSp0e-DARUcf-RJLb9c" alt=" 3/8페이지" aria-hidden="true"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"><h2 class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-tJHJj" tabindex="0">&nbsp;4/8페이지</h2><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 47.0588%; top: 95.7071%; width: 7.02614%; height: 1.76768%;">252
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 28.5948%; top: 5.80808%; width: 46.8954%; height: 1.38889%;">Understanding the difficulty of training deep feedforward neural networks
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 9.34343%; width: 38.7255%; height: 1.51515%;">then those found with symmetric activation functions, as
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 10.8586%; width: 16.1765%; height: 1.51515%;">can be seen in figure 11.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 13.8889%; width: 33.0065%; height: 1.51515%;">3.2 Experiments with the Hyperbolic tangent
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 16.2879%; width: 38.7255%; height: 1.51515%;">As discussed above, the hyperbolic tangent networks do not
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 17.803%; width: 38.7255%; height: 3.0303%;">suffer from the kind of saturation behavior of the top hid-
den layer observed with sigmoid networks, because of its
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 20.8333%; width: 38.5621%; height: 1.51515%;">symmetry around 0. However, with our standard weight
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 22.7273%; width: 10.2941%; height: 1.26263%;">initialization U
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 22.3856%; top: 22.096%; width: 0.816993%; height: 2.65152%;">h
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 23.0392%; top: 23.2323%; width: 3.26797%; height: 1.38889%;">− √
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 25.3268%; top: 22.4747%; width: 0.980392%; height: 0.883838%;">1
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 25.817%; top: 23.6111%; width: 1.14379%; height: 0.757576%;">n
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 26.7974%; top: 23.3586%; width: 2.45098%; height: 1.26263%;">, √
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 28.4314%; top: 22.4747%; width: 0.816993%; height: 0.883838%;">1
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 28.7582%; top: 23.6111%; width: 1.14379%; height: 0.757576%;">n
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 29.7386%; top: 22.096%; width: 0.816993%; height: 2.65152%;">i
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 22.7273%; width: 38.7255%; height: 3.28283%;">, we observe a sequentially oc-
curring saturation phenomenon starting with layer 1 and
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 26.0101%; width: 38.5621%; height: 1.51515%;">propagating up in the network, as illustrated in Figure 3.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 27.5253%; width: 32.1895%; height: 1.51515%;">Why this is happening remains to be understood.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 52.904%; width: 38.7255%; height: 1.51515%;">Figure 3: Top:98 percentiles (markers alone) and standard
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 54.4192%; width: 39.0523%; height: 1.38889%;">deviation (solid lines with markers) of the distribution of
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 55.9343%; width: 38.5621%; height: 1.51515%;">the activation values for the hyperbolic tangent networks in
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 57.4495%; width: 38.7255%; height: 3.0303%;">the course of learning. We see the first hidden layer satu-
rating first, then the second, etc. Bottom: 98 percentiles
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 60.4798%; width: 38.5621%; height: 1.38889%;">(markers alone) and standard deviation (solid lines with
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 61.8687%; width: 38.7255%; height: 3.0303%;">markers) of the distribution of activation values for the soft-
sign during learning. Here the different layers saturate less
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 64.899%; width: 12.5817%; height: 1.51515%;">and do so together.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 67.803%; width: 25%; height: 1.51515%;">3.3 Experiments with the Softsign
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 70.202%; width: 38.7255%; height: 1.64141%;">The softsign x/(1+|x|) is similar to the hyperbolic tangent
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 71.7172%; width: 38.7255%; height: 1.51515%;">but might behave differently in terms of saturation because
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 73.2323%; width: 38.7255%; height: 3.0303%;">of its smoother asymptotes (polynomial instead of expo-
nential). We see on Figure 3 that the saturation does not
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 76.2626%; width: 38.7255%; height: 3.0303%;">occur one layer after the other like for the hyperbolic tan-
gent. It is faster at the beginning and then slow, and all
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 79.2929%; width: 29.5752%; height: 1.51515%;">layers move together towards larger weights.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 81.5657%; width: 38.7255%; height: 1.51515%;">We can also see at the end of training that the histogram
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 83.0808%; width: 38.7255%; height: 1.51515%;">of activation values is very different from that seen with
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 84.596%; width: 38.7255%; height: 1.51515%;">the hyperbolic tangent (Figure 4). Whereas the latter yields
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 86.1111%; width: 38.7255%; height: 1.51515%;">modes of the activations distribution mostly at the extremes
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.2549%; top: 87.6263%; width: 38.5621%; height: 1.51515%;">(asymptotes -1 and 1) or around 0, the softsign network has
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 89.1414%; width: 38.7255%; height: 1.38889%;">modes of activations around its knees (between the linear
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 90.6566%; width: 38.7255%; height: 1.51515%;">regime around 0 and the flat regime around -1 and 1). These
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 12.0915%; top: 92.1717%; width: 38.7255%; height: 1.51515%;">are the areas where there is substantial non-linearity but
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 9.34343%; width: 24.5098%; height: 1.51515%;">where the gradients would flow well.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 34.4697%; width: 38.7255%; height: 1.51515%;">Figure 4: Activation values normalized histogram at the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 35.9848%; width: 38.7255%; height: 1.51515%;">end of learning, averaged across units of the same layer and
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 37.5%; width: 38.8889%; height: 4.54545%;">across 300 test examples. Top: activation function is hyper-
bolic tangent, we see important saturation of the lower lay-
ers. Bottom: activation function is softsign, we see many
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 42.0455%; width: 38.5621%; height: 1.38889%;">activation values around (-0.6,-0.8) and (0.6,0.8) where the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 43.5606%; width: 25.6536%; height: 1.13636%;">units do not saturate but are non-linear.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 46.2121%; width: 38.7255%; height: 1.76768%;">4 Studying Gradients and their Propagation
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 49.1162%; width: 22.8758%; height: 1.26263%;">4.1 Effect of the Cost Function
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 51.5152%; width: 38.7255%; height: 1.51515%;">We have found that the logistic regression or conditional
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 52.904%; width: 38.7255%; height: 1.64141%;">log-likelihood cost function (− log P(y|x) coupled with
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 54.5455%; width: 38.5621%; height: 1.51515%;">softmax outputs) worked much better (for classification
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 56.0606%; width: 38.7255%; height: 3.0303%;">problems) than the quadratic cost which was tradition-
ally used to train feedforward neural networks (Rumelhart
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 59.0909%; width: 38.7255%; height: 1.38889%;">et al., 1986). This is not a new observation (Solla et al.,
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 60.6061%; width: 38.5621%; height: 1.51515%;">1988) but we find it important to stress here. We found that
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 62.1212%; width: 38.7255%; height: 4.41919%;">the plateaus in the training criterion (as a function of the pa-
rameters) are less present with the log-likelihood cost func-
tion. We can see this on Figure 5, which plots the training
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 66.5404%; width: 38.7255%; height: 3.0303%;">criterion as a function of two weights for a two-layer net-
work (one hidden layer) with hyperbolic tangent units, and
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 69.5707%; width: 38.7255%; height: 1.51515%;">a random input and target signal. There are clearly more
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.4314%; top: 71.0859%; width: 25.6536%; height: 1.51515%;">severe plateaus with the quadratic cost.
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 73.9899%; width: 22.0588%; height: 1.26263%;">4.2 Gradients at initialization
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 76.3889%; width: 31.8627%; height: 1.26263%;">4.2.1 Theoretical Considerations and a New
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 58.1699%; top: 77.904%; width: 17.9739%; height: 1.26263%;">Normalized Initialization
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 80.0505%; width: 38.7255%; height: 1.51515%;">We study the back-propagated gradients, or equivalently
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 81.5657%; width: 38.7255%; height: 1.51515%;">the gradient of the cost function on the inputs biases at each
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 83.0808%; width: 38.7255%; height: 1.51515%;">layer. Bradley (2009) found that back-propagated gradients
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 84.596%; width: 38.7255%; height: 1.51515%;">were smaller as one moves from the output layer towards
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 86.1111%; width: 38.7255%; height: 1.51515%;">the input layer, just after initialization. He studied networks
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 87.6263%; width: 38.7255%; height: 1.51515%;">with linear activation at each layer, finding that the variance
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 89.1414%; width: 38.7255%; height: 3.0303%;">of the back-propagated gradients decreases as we go back-
wards in the network. We will also start by studying the
</p><p class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-eEGnhe" style="left: 53.268%; top: 92.1717%; width: 9.47712%; height: 1.51515%;">linear regime.
</p></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div><img src="blob:https://drive.google.com/5571a82a-3a43-48fd-a3eb-07d8aaaaabdb" class="ndfHFb-c4YZDc-cYSp0e-DARUcf-RJLb9c" alt=" 4/8페이지" aria-hidden="true"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf" style="padding-bottom: 129.438%;"><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-PLDbbf ndfHFb-c4YZDc-AHmuwe-wcotoc-zTETae"></div><div class="ndfHFb-c4YZDc-cYSp0e-DARUcf-Df1ZY-bN97Pc-haAclf"></div><div class="ndfHFb-c4YZDc-cYSp0e-wxLEad-sn54Q" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-gvZm2b"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-vWsuo-fmcmS-G0jgYd"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-IDNFyf ndfHFb-c4YZDc-cYSp0e-oYxtQd-gvZm2b"></div></div></div><span class="ndfHFb-c4YZDc-cYSp0e-AznF2e-DTMEae" tabindex="0"></span></div><div class="ndfHFb-c4YZDc-n5VRYe-ma6Yeb" style="display: none;"></div><div class="ndfHFb-c4YZDc-n5VRYe-AeOLfc" style="display: none;"></div><div class="ndfHFb-c4YZDc-n5VRYe-cGMI2b" style="display: none;"></div><div class="ndfHFb-c4YZDc-n5VRYe-hOcTPc" style="display: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe" aria-hidden="true"><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-haAclf"><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-YPqjbf-haAclf"><input class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-YPqjbf V67aGc-YPqjbf-V67aGc" placeholder="문서에서 찾기" aria-label="문서에서 찾기" tabindex="-1"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-NnAfwf-haAclf"><span class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-NnAfwf"></span></div></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-E7ORLb ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-SKd3Ne VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="이전 찾기" data-tooltip="이전 찾기" style="user-select: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-tJiF1e ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-SKd3Ne VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="다음 찾기" data-tooltip="다음 찾기" style="user-select: none;"></div><div class="ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-TvD9Pc ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-vWsuo-fmcmS-xxlfEe-SKd3Ne VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="찾기 창 닫기" data-tooltip="찾기 창 닫기" style="user-select: none;"></div></div></div><div style="display:none" id="drive-active-item-info">{"id": "\/viewerng\/upload?ds\x3dAON1mFybx8Ki_RrJckkFxTA4oY73Z8UYeFdoZ1sJCBbCczv315eM2JyxMTHmrWlMvpkn83nLpuLT7SaP3gs5JzWoNPntHsTqm3C3AO3x2ouH4llWQTfK45D0wCmAK6urGdhLXJ9jvpqbUXYdKZxh4AB2JAFi9Ni0mePvzX90xMsIcAQbnZVi0SK7CUVw_ei0653MJAjAK93ahhRaMqCdYQ9NPgwWcBKSG1yxORb73W_ZTEjw7b59bNqUlYECYfUsLVHCu2ziC07oalzAAyP3F2EkwZ5Stu-7IQ%3D%3D\x26ck\x3dlantern\x26authuser\x3d0\x26p\x3dproj", "title": "glorot10a.pdf", "mimeType": "application\/pdf"}</div></div></div><div class="ndfHFb-c4YZDc-Wrql6b" role="toolbar" style="opacity: 0;"><div class="ndfHFb-c4YZDc-Wrql6b-SmKAyb" style="margin-right: 12px; padding-left: 12px;"><div class="ndfHFb-c4YZDc-Wrql6b-hOcTPc" style="left: 12px;"><div class="ndfHFb-c4YZDc-Wrql6b-Bz112c" tabindex="-1" role="img" aria-label="PDF 아이콘" style="background-image: url(&quot;//ssl.gstatic.com/docs/doclist/images/mediatype/icon_3_pdf_x32.png&quot;); background-position: left top; background-repeat: no-repeat;"></div><div class="ndfHFb-c4YZDc-Wrql6b-jfdpUb" tabindex="-1"><div class="ndfHFb-c4YZDc-Wrql6b-V1ur5d" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6">glorot10a.pdf</div><div class="ndfHFb-c4YZDc-Wrql6b-V1ur5d ndfHFb-c4YZDc-Wrql6b-V1ur5d-hpYHOb">glorot10a.pdf</div><div class="ndfHFb-c4YZDc-Wrql6b-K4efff-V1ur5d" style="display: none;"></div><div class="ndfHFb-c4YZDc-Wrql6b-K4efff-V1ur5d ndfHFb-c4YZDc-Wrql6b-K4efff-V1ur5d-hpYHOb"></div></div><div class="ndfHFb-c4YZDc-Wrql6b-C7uZwb-b0t70b"></div></div><div class="ndfHFb-c4YZDc-Wrql6b-DdWCyb-b0t70b"><div class="ndfHFb-c4YZDc-Wrql6b-FNFY6c-J42Xof-qMHh7d"><div class="ndfHFb-c4YZDc-Wrql6b-FNFY6c ndfHFb-c4YZDc-to915-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-Wrql6b-FNFY6c-BP2Omd-qMHh7d" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-hidden="false" aria-label="Google 문서(으)로 열기" style="user-select: none;" tabindex="0" data-tooltip="Google 문서(으)로 열기"><div class="ndfHFb-c4YZDc-FNFY6c-DWWcKd-Bz112c" style="background-image: url(&quot;https://ssl.gstatic.com/docs/doclist/images/mediatype/icon_1_document_x64.png&quot;); background-position: left top; background-repeat: no-repeat;"></div><div class="ndfHFb-c4YZDc-FNFY6c-V67aGc">Google 문서(으)로 열기</div></div><div class="ndfHFb-c4YZDc-Wrql6b-PlOyMe ndfHFb-c4YZDc-to915-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-Wrql6b-FNFY6c-BP2Omd-qMHh7d" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-hidden="true" style="user-select: none; display: none;"><div class="ndfHFb-c4YZDc-Wrql6b-PlOyMe-bN97Pc">추출</div><div class="ndfHFb-c4YZDc-Wrql6b-HDMZaf-Bz112c"><div class="ndfHFb-aZ2wEe"><div class="ndfHFb-vyDMJf-aZ2wEe auswjd"><div class="aZ2wEe-pbTTYe aZ2wEe-v3pZbf"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-oq6NAc"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-gS7Ybc"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-nllRtd"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div></div></div></div></div><div class="ndfHFb-c4YZDc-Wrql6b-qMHh7d ndfHFb-c4YZDc-to915-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-Wrql6b-qMHh7d-yolsp" role="button" aria-expanded="false" aria-haspopup="true" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="연결 앱" data-tooltip="연결 앱" style="user-select: none;" aria-hidden="false" tabindex="0"><div class="ndfHFb-c4YZDc-Wrql6b-FNFY6c-hgDUwe"></div><div class="ndfHFb-c4YZDc-Wrql6b-qMHh7d-SmKAyb"><div class="ndfHFb-c4YZDc-Wrql6b-qMHh7d-fmcmS" tabindex="-1">연결 프로그램</div><div class="ndfHFb-c4YZDc-Wrql6b-xl07Ob-LgbsSe-hFsbo"><div class="ndfHFb-c4YZDc-Bz112c"></div></div></div></div></div><div class="ndfHFb-c4YZDc-Wrql6b-C7uZwb-b0t70b"></div></div><div class="ndfHFb-c4YZDc-Wrql6b-AeOLfc-b0t70b"><div class="ndfHFb-c4YZDc-GSQQnc-LgbsSe ndfHFb-c4YZDc-to915-LgbsSe" aria-label="크게 보기" style="display: none;"><div class="ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-DH6Rkf-Bz112c"></div></div><div class="ndfHFb-c4YZDc-Wrql6b-LQLjdd"><div class="ndfHFb-c4YZDc-Wrql6b-htvI8d-wcotoc-ndfHFb ndfHFb-c4YZDc-to915-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe" role="button" tabindex="0" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="드라이브에 저장" data-tooltip="드라이브에 저장" style="user-select: none;"><div class="ndfHFb-c4YZDc-Wrql6b-htvI8d-wcotoc-ndfHFb-Bz112c"><div class="ndfHFb-aZ2wEe"><div class="ndfHFb-vyDMJf-aZ2wEe auswjd"><div class="aZ2wEe-pbTTYe aZ2wEe-v3pZbf"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-oq6NAc"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-gS7Ybc"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div><div class="aZ2wEe-pbTTYe aZ2wEe-nllRtd"><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-LK5yu"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-pehrl-TpMipd"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div><div class="aZ2wEe-LkdAo-e9ayKc aZ2wEe-qwU8Me"><div class="aZ2wEe-LkdAo aZ2wEe-hj4D6d"></div></div></div></div></div></div></div><div class="ndfHFb-c4YZDc-Wrql6b-C7uZwb-b0t70b"><div class="ndfHFb-c4YZDc-to915-LgbsSe ndfHFb-c4YZDc-C7uZwb-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-C7uZwb-LgbsSe-SfQLQb-Bz112c" role="button" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-disabled="false" aria-hidden="false" aria-label="인쇄" style="user-select: none;" tabindex="0" data-tooltip="인쇄"><div class="ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-C7uZwb-LgbsSe-Bz112c ndfHFb-c4YZDc-PEFSMe-Bz112c"></div></div><div class="ndfHFb-c4YZDc-to915-LgbsSe ndfHFb-c4YZDc-C7uZwb-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe ndfHFb-c4YZDc-C7uZwb-LgbsSe-SfQLQb-Bz112c" role="button" tabindex="0" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="원본 열기" data-tooltip="원본 열기" style="user-select: none;"><div class="ndfHFb-c4YZDc-Bz112c ndfHFb-c4YZDc-C7uZwb-LgbsSe-Bz112c ndfHFb-c4YZDc-FNFY6c-bEDTcc-oxvKad-Bz112c"></div></div><div class="ndfHFb-c4YZDc-z5C9Gb-LgbsSe ndfHFb-c4YZDc-to915-LgbsSe VIpgJd-TzA9Ye-eEGnhe ndfHFb-c4YZDc-LgbsSe" role="button" aria-expanded="false" aria-haspopup="true" data-tooltip-unhoverable="true" data-tooltip-delay="500" data-tooltip-class="ndfHFb-c4YZDc-tk3N6e-suEOdc" data-tooltip-align="b,c" data-tooltip-offset="-6" aria-label="추가 작업" data-tooltip="추가 작업" aria-hidden="true" style="user-select: none; display: none;"><div class="ndfHFb-c4YZDc-Bz112c"></div></div></div></div><div class="ndfHFb-c4YZDc-n1UuX-Bz112c" title="김가빈 
(supersynaps12@gmail.com)"><img src="./softsignglorot10a.pdf_files/photo.jpg" class="ndfHFb-c4YZDc-n1UuX-RJLb9c" alt="김가빈 
(supersynaps12@gmail.com)" tabindex="0"></div></div></div></div></div><span class="ndfHFb-c4YZDc-AznF2e-DTMEae" tabindex="0" aria-hidden="true" style=""></span><iframe id="apiproxy13aba19a03c2f91f0bc09b7e73fe8899d69dc0f00.48755312" name="apiproxy13aba19a03c2f91f0bc09b7e73fe8899d69dc0f00.48755312" src="./softsignglorot10a.pdf_files/proxy.html" tabindex="-1" aria-hidden="true" style="width: 1px; height: 1px; position: absolute; top: -100px; display: none;"></iframe><iframe id="apiproxy43ab456e5bb339eef252b3db2c91e52bb0ba2b3e0.4172672762" name="apiproxy43ab456e5bb339eef252b3db2c91e52bb0ba2b3e0.4172672762" src="./softsignglorot10a.pdf_files/proxy(1).html" tabindex="-1" aria-hidden="true" style="width: 1px; height: 1px; position: absolute; top: -100px; display: none;"></iframe><div aria-live="polite" aria-atomic="true" style="position: absolute; top: -1000px; height: 1px; overflow: hidden;">glorot10a.pdf 표시 중입니다.</div><div class="tk3N6e-suEOdc ndfHFb-c4YZDc-tk3N6e-suEOdc tk3N6e-suEOdc-ZYIfFd" role="tooltip" style="left: 591px; top: 54px;"><div class="tk3N6e-suEOdc-W095bf">원본 열기</div><div class="tk3N6e-suEOdc-hFsbo tk3N6e-suEOdc-d6mlqf" style="left: 34px;"><div class="tk3N6e-suEOdc-jQ8oHc"></div><div class="tk3N6e-suEOdc-ez0xG"></div></div></div></body></html>